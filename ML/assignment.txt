Divided into sets :
['a','m','n','s','t','o'],
['b','e','c','x'],
['h','k','u','v'],
['g','q'],
['d','r','p'],
['f'],
['l'],
['i'],
['w'],
['y']
Features :
pinchStrength,grabStrength,thumb_meta_proxi,thumb_proxi_inter,index_meta_proxi,
index_proxi_inter,middle_meta_proxi,middle_proxi_inter,ring_meta_proxi,ring_proxi_inter,
pinky_meta_proxi,pinky_proxi_inter,thumb_index,index_middle,middle_ring,ring_pinky,
palm_direction,thumb_direction_x,thumb_direction_y,thumb_direction_z,index_direction_x,
index_direction_y,index_direction_z,middle_direction_x,middle_direction_y,middle_direction_z,
ring_direction_x,ring_direction_y,ring_direction_z,pinky_direction_x,pinky_direction_y,
pinky_direction_z,label

Randomly 1 letter was taken from each set and a model trained with them then tested
with test data
Done 100 times and taken average
Data was normalized by using preprocessing.scale() method of sklearn
SVM Accuracy : 99.7%
Dtree Accuracy : 98.7%
KNN Accuracy : 99.3%
LDA Accuracy : 98.3%
SGD Accuracy : 99.2%

--------------------------------------------------------------------------------------
Set 1 : 90.1%
Set 2 : 99.7%
Set 3 : 99.1%
Set 4 : 99.5%

When set is trained as whole
ie : Model that only classifies which set a letter belongs to
SVM Accuracy : 99.2%
Dtree Accuracy : 97.9%
KNN Accuracy : 99.0%
LDA Accuracy : 95.1%
SGD Accuracy : 97.3%
-------------------------------------------------------------------------------------
Confusion matrices :
Set 4 :
[ 0.99175  0.       0.00825]
[ 0.  1.  0.]
[ 0.00742857  0.          0.99257143]


Set 3 :
[[ 0.99257143  0.00742857  0.          0.        ]
 [ 0.          0.98771429  0.          0.01228571]
 [ 0.00142857  0.          0.99857143  0.        ]
 [ 0.          0.012       0.          0.988     ]]


Set 2 :
[[ 0.99885714  0.          0.00114286]
 [ 0.          1.          0.        ]
 [ 0.00756098  0.          0.99243902]]


Set 1 :
[[  9.96000000e-01   0.00000000e+00   3.71428571e-03   2.85714286e-04
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00]
 [  0.00000000e+00   9.90857143e-01   2.57142857e-03   1.14285714e-03
    0.00000000e+00   2.00000000e-03   2.85714286e-03   5.71428571e-04
    0.00000000e+00]
 [  1.42857143e-03   0.00000000e+00   7.32000000e-01   1.71714286e-01
    2.85714286e-04   0.00000000e+00   7.48571429e-02   1.97142857e-02
    0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   1.26571429e-01   7.65714286e-01
    0.00000000e+00   0.00000000e+00   5.54285714e-02   5.22857143e-02
    0.00000000e+00]
 [  6.00000000e-03   0.00000000e+00   2.57142857e-03   3.42857143e-03
    9.86285714e-01   0.00000000e+00   1.71428571e-03   0.00000000e+00
    0.00000000e+00]
 [  0.00000000e+00   7.42857143e-03   0.00000000e+00   0.00000000e+00
    2.28571429e-03   9.76000000e-01   1.14285714e-02   0.00000000e+00
    2.85714286e-03]
 [  2.80555556e-02   6.38888889e-03   4.94444444e-02   5.27777778e-02
    3.88888889e-03   2.22222222e-03   8.33055556e-01   2.41666667e-02
    0.00000000e+00]
 [  0.00000000e+00   1.71428571e-03   5.62857143e-02   9.34285714e-02
    2.85714286e-03   0.00000000e+00   6.28571429e-03   8.39428571e-01
    0.00000000e+00]
 [  2.85714286e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00
    1.08571429e-02   0.00000000e+00   0.00000000e+00   1.71428571e-03
    9.87142857e-01]]
